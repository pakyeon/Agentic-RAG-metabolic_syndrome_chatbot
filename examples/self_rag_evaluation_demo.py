"""
Self-RAG í‰ê°€ ì‹¤ì „ ì˜ˆì œ

ì‹¤ì œ RAG ì‹œìŠ¤í…œì—ì„œ Self-RAG í‰ê°€ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œ
"""

import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.evaluation import create_evaluator


def simulate_rag_pipeline():
    """
    ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

    íë¦„:
    0. [Retrieve]: ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
    1. ì‚¬ìš©ì ì§ˆë¬¸
    2. VectorDB ê²€ìƒ‰
    3. ISREL: ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€
    4. ì™¸ë¶€ ê²€ìƒ‰ í•„ìš” íŒë‹¨
    5. LLM ë‹µë³€ ìƒì„±
    6. ISSUP & ISUSE: ë‹µë³€ í’ˆì§ˆ í‰ê°€
    7. ì¬ìƒì„± í•„ìš” íŒë‹¨
    """

    print("=" * 70)
    print("Self-RAG ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜")
    print("=" * 70)

    # í‰ê°€ì ìƒì„±
    evaluator = create_evaluator()

    # 0ë‹¨ê³„: [Retrieve] - ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
    query = "ëŒ€ì‚¬ì¦í›„êµ° í™˜ìì—ê²Œ ê¶Œì¥ë˜ëŠ” ì‹ë‹¨ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    print(f"\n[0ë‹¨ê³„] [Retrieve] - ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨")
    print(f"  ì§ˆë¬¸: {query}")

    retrieve_result = evaluator.evaluate_retrieve_need(query)
    print(f"  íŒë‹¨: {retrieve_result.decision}")
    print(f"  ì´ìœ : {retrieve_result.reason}")

    if retrieve_result.decision == "no":
        print("  â†’ LLM ìì²´ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥. ê²€ìƒ‰ ê±´ë„ˆëœ€.")
        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ ë°”ë¡œ LLMì—ê²Œ ë‹µë³€ ìš”ì²­
        return

    print("  â†’ ê²€ìƒ‰ ì§„í–‰")

    # 1ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸
    print(f"\n[1ë‹¨ê³„] ì‚¬ìš©ì ì§ˆë¬¸ í™•ì¸")
    print(f"  {query}")

    # 2ë‹¨ê³„: VectorDBì—ì„œ ë¬¸ì„œ ê²€ìƒ‰ (ì‹œë®¬ë ˆì´ì…˜)
    print(f"\n[2ë‹¨ê³„] VectorDB ê²€ìƒ‰")
    retrieved_documents = [
        """ëŒ€ì‚¬ì¦í›„êµ° í™˜ìëŠ” ì €ì—¼ì‹, ì €ì§€ë°©ì‹ì„ ì„­ì·¨í•´ì•¼ í•©ë‹ˆë‹¤. 
        ì±„ì†Œì™€ ê³¼ì¼ì„ ì¶©ë¶„íˆ ì„­ì·¨í•˜ê³ , ê°€ê³µì‹í’ˆì„ í”¼í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.""",
        """ê·œì¹™ì ì¸ ìš´ë™ì´ ëŒ€ì‚¬ì¦í›„êµ° ê´€ë¦¬ì— ì¤‘ìš”í•©ë‹ˆë‹¤. 
        ì£¼ 3-5íšŒ, íšŒë‹¹ 30ë¶„ ì´ìƒì˜ ìœ ì‚°ì†Œ ìš´ë™ì„ ê¶Œì¥í•©ë‹ˆë‹¤.""",
        """ëŒ€ì‚¬ì¦í›„êµ° í™˜ìëŠ” í˜ˆë‹¹ ê´€ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.
        ì €GI ì‹í’ˆì„ ì„ íƒí•˜ê³ , ì‹ì‚¬ëŠ” ê·œì¹™ì ìœ¼ë¡œ í•˜ì„¸ìš”.""",
    ]
    print(f"  ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_documents)}ê°œ")

    # 3ë‹¨ê³„: ISREL - ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€
    print(f"\n[3ë‹¨ê³„] ISREL - ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€")
    overall_eval = evaluator.evaluate_documents(
        query, retrieved_documents, min_relevant_docs=2
    )

    relevant_docs = []
    for i, doc_eval in enumerate(overall_eval.document_evaluations, 1):
        relevance = doc_eval.relevance.relevance
        print(f"  ë¬¸ì„œ {i}: {relevance}")
        if relevance == "relevant":
            relevant_docs.append(doc_eval.document_content)

    # 4ë‹¨ê³„: ì™¸ë¶€ ê²€ìƒ‰ í•„ìš” íŒë‹¨
    print(f"\n[4ë‹¨ê³„] ì™¸ë¶€ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨")
    print(f"  í•„ìš” ì—¬ë¶€: {overall_eval.should_retrieve_external}")
    print(f"  ì‚¬ìœ : {overall_eval.reason}")

    if overall_eval.should_retrieve_external:
        print("  â†’ Tavily ì™¸ë¶€ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ Tavily Tool í˜¸ì¶œ
        external_doc = (
            "ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼: ëŒ€ì‚¬ì¦í›„êµ° ì‹ë‹¨ì€ DASH ì‹ë‹¨ì„ ë”°ë¥´ëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤."
        )
        relevant_docs.append(external_doc)
        print(f"  ì™¸ë¶€ ë¬¸ì„œ ì¶”ê°€: {len(relevant_docs)}ê°œ â†’ {len(relevant_docs)}ê°œ")

    # 5ë‹¨ê³„: LLM ë‹µë³€ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
    print(f"\n[5ë‹¨ê³„] LLM ë‹µë³€ ìƒì„±")
    generated_answer = """
    ëŒ€ì‚¬ì¦í›„êµ° í™˜ìì—ê²Œ ê¶Œì¥ë˜ëŠ” ì‹ë‹¨ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    
    1. ì €ì—¼ì‹, ì €ì§€ë°©ì‹ ì„­ì·¨
    2. ì±„ì†Œì™€ ê³¼ì¼ ì¶©ë¶„íˆ ì„­ì·¨
    3. ê°€ê³µì‹í’ˆ í”¼í•˜ê¸°
    4. ì €GI(í˜ˆë‹¹ì§€ìˆ˜) ì‹í’ˆ ì„ íƒ
    5. ê·œì¹™ì ì¸ ì‹ì‚¬
    
    ì´ëŸ¬í•œ ì‹ìŠµê´€ê³¼ í•¨ê»˜ ì£¼ 3-5íšŒ, íšŒë‹¹ 30ë¶„ ì´ìƒì˜ 
    ìœ ì‚°ì†Œ ìš´ë™ì„ ë³‘í–‰í•˜ë©´ ë”ìš± íš¨ê³¼ì ì…ë‹ˆë‹¤.
    """
    print(f"  ë‹µë³€ ìƒì„± ì™„ë£Œ")
    print(f"  ë‹µë³€ ê¸¸ì´: {len(generated_answer)}ì")

    # 6ë‹¨ê³„: ISSUP & ISUSE - ë‹µë³€ í’ˆì§ˆ í‰ê°€
    print(f"\n[6ë‹¨ê³„] ISSUP & ISUSE - ë‹µë³€ í’ˆì§ˆ í‰ê°€")
    answer_quality = evaluator.evaluate_answer_quality(
        query, generated_answer, relevant_docs
    )

    print(f"  ISSUP (ì§€ì§€ë„):")
    for i, support_result in enumerate(answer_quality["support_results"], 1):
        print(f"    ë¬¸ì„œ {i}: {support_result.support}")

    print(f"  ISUSE (ìœ ìš©ì„±): {answer_quality['usefulness'].score}/5")
    print(f"  ì™„ì „íˆ ë’·ë°›ì¹¨ë˜ëŠ” ë¬¸ì„œ ìˆ˜: {answer_quality['fully_supported_count']}")

    # 7ë‹¨ê³„: ì¬ìƒì„± í•„ìš” íŒë‹¨
    print(f"\n[7ë‹¨ê³„] ì¬ìƒì„± í•„ìš”ì„± íŒë‹¨")
    should_regenerate = answer_quality["should_regenerate"]
    print(f"  ì¬ìƒì„± í•„ìš”: {should_regenerate}")

    if should_regenerate:
        print("  â†’ ë‹µë³€ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. ì¬ìƒì„± ë˜ëŠ” ì¶”ê°€ ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        print("  â†’ ë‹µë³€ í’ˆì§ˆì´ ì¶©ë¶„í•©ë‹ˆë‹¤. ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # 8ë‹¨ê³„: ìµœì¢… ê²°ê³¼
    print(f"\n[8ë‹¨ê³„] ìµœì¢… ê²°ê³¼")
    print(f"\n{generated_answer}")

    print("\n" + "=" * 70)
    print("íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    print("=" * 70)


def demonstrate_edge_cases():
    """
    ì—£ì§€ ì¼€ì´ìŠ¤ ì‹œì—°
    """
    print("\n\n" + "=" * 70)
    print("ì—£ì§€ ì¼€ì´ìŠ¤ ì‹œì—°")
    print("=" * 70)

    evaluator = create_evaluator()

    # Case 0: [Retrieve] - ê²€ìƒ‰ ë¶ˆí•„ìš”í•œ ê²½ìš°
    print("\n[Case 0] ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ì¼ë°˜ ì§ˆë¬¸")
    general_query = "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œìš”?"
    retrieve_result = evaluator.evaluate_retrieve_need(general_query)
    print(f"  ì§ˆë¬¸: {general_query}")
    print(f"  ê²€ìƒ‰ í•„ìš”: {retrieve_result.decision}")
    print(f"  ì´ìœ : {retrieve_result.reason}")

    # Case 1: ëª¨ë“  ë¬¸ì„œê°€ ê´€ë ¨ ì—†ëŠ” ê²½ìš°
    print("\n[Case 1] ëª¨ë“  ê²€ìƒ‰ ë¬¸ì„œê°€ ê´€ë ¨ ì—†ëŠ” ê²½ìš°")
    query = "ëŒ€ì‚¬ì¦í›„êµ°ì˜ ì§„ë‹¨ ê¸°ì¤€ì€?"
    irrelevant_docs = [
        "ê°ê¸°ëŠ” ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼ì…ë‹ˆë‹¤.",
        "ë¹„íƒ€ë¯¼ DëŠ” ë¼ˆ ê±´ê°•ì— ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ëŠ” ì •ì‹  ê±´ê°•ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.",
    ]

    eval_result = evaluator.evaluate_documents(
        query, irrelevant_docs, min_relevant_docs=1
    )
    print(f"  ì™¸ë¶€ ê²€ìƒ‰ í•„ìš”: {eval_result.should_retrieve_external}")
    print(f"  ì‚¬ìœ : {eval_result.reason}")

    # Case 2: ë‹µë³€ì´ ë¬¸ì„œë¡œ ë’·ë°›ì¹¨ë˜ì§€ ì•ŠëŠ” ê²½ìš°
    print("\n[Case 2] ë‹µë³€ì´ ë¬¸ì„œë¡œ ë’·ë°›ì¹¨ë˜ì§€ ì•ŠëŠ” ê²½ìš°")
    query = "ëŒ€ì‚¬ì¦í›„êµ° ì˜ˆë°©ë²•ì€?"
    document = "ëŒ€ì‚¬ì¦í›„êµ°ì€ ë³µë¶€ ë¹„ë§Œê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤."
    wrong_answer = "ëŒ€ì‚¬ì¦í›„êµ°ì€ íŠ¹ë³„í•œ ì•½ë¬¼ ì¹˜ë£Œê°€ í•„ìš”í•©ë‹ˆë‹¤."

    support_result = evaluator.evaluate_support(query, document, wrong_answer)
    print(f"  ì§€ì§€ë„ í‰ê°€: {support_result.support}")

    # Case 3: ë‹µë³€ì´ ìœ ìš©í•˜ì§€ ì•Šì€ ê²½ìš°
    print("\n[Case 3] ë‹µë³€ì´ ìœ ìš©í•˜ì§€ ì•Šì€ ê²½ìš°")
    query = "ëŒ€ì‚¬ì¦í›„êµ° ì¹˜ë£Œ ë°©ë²•ì„ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”"
    bad_answer = "ëŒ€ì‚¬ì¦í›„êµ°ì€ ë³µì¡í•œ ì§ˆí™˜ì…ë‹ˆë‹¤."

    usefulness_result = evaluator.evaluate_usefulness(query, bad_answer)
    print(f"  ìœ ìš©ì„± ì ìˆ˜: {usefulness_result.score}/5")


if __name__ == "__main__":
    from dotenv import load_dotenv

    load_dotenv()
    print("\nğŸ¯ Self-RAG í‰ê°€ ì‹¤ì „ ì˜ˆì œ\n")

    try:
        # ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜
        simulate_rag_pipeline()

        # ì—£ì§€ ì¼€ì´ìŠ¤ ì‹œì—°
        demonstrate_edge_cases()

        print("\nâœ… ì˜ˆì œ ì‹¤í–‰ ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâŒ ì˜ˆì œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
