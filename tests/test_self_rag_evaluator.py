"""
Self-RAG í‰ê°€ì í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ
"""

import sys
import os
import pytest

if not os.getenv("OPENAI_API_KEY"):
    pytest.skip("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì•„ Self-RAG í‰ê°€ì í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.", allow_module_level=True)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.evaluation.self_rag_evaluator import create_evaluator, SelfRAGEvaluator


def test_retrieve():
    """[Retrieve] í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("[Retrieve] ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    evaluator = create_evaluator()

    # ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ë“¤
    queries_need_retrieval = [
        "ëŒ€ì‚¬ì¦í›„êµ°ì˜ ìµœì‹  ì¹˜ë£Œ ê°€ì´ë“œë¼ì¸ì€?",
        "2024ë…„ ëŒ€ì‚¬ì¦í›„êµ° ìœ ë³‘ë¥  í†µê³„ëŠ”?",
        "ëŒ€ì‚¬ì¦í›„êµ° ì§„ë‹¨ ê¸°ì¤€ 5ê°€ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    ]

    # ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ì§ˆë¬¸ë“¤
    queries_no_retrieval = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œìš”?",
        "1+1ì€?",
    ]

    print("\n[ê²€ìƒ‰ í•„ìš”í•œ ì§ˆë¬¸ë“¤]")
    for query in queries_need_retrieval:
        result = evaluator.evaluate_retrieve_need(query)
        print(f"\nì§ˆë¬¸: {query}")
        print(f"  íŒë‹¨: {result.should_retrieve}")
        print(f"  ë‚œì´ë„: {result.difficulty}")
        print(f"  í‰ê°€ ë¬¸ì„œ ìˆ˜: {result.documents_to_evaluate}")
        print(f"  ì´ìœ : {result.reason}")

    print("\n\n[ê²€ìƒ‰ ë¶ˆí•„ìš”í•œ ì§ˆë¬¸ë“¤]")
    for query in queries_no_retrieval:
        result = evaluator.evaluate_retrieve_need(query)
        print(f"\nì§ˆë¬¸: {query}")
        print(f"  íŒë‹¨: {result.should_retrieve}")
        print(f"  ë‚œì´ë„: {result.difficulty}")
        print(f"  í‰ê°€ ë¬¸ì„œ ìˆ˜: {result.documents_to_evaluate}")
        print(f"  ì´ìœ : {result.reason}")


def test_isrel():
    """ISREL (ê´€ë ¨ì„±) í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ISREL (ê´€ë ¨ì„±) í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    evaluator = create_evaluator()

    query = "ëŒ€ì‚¬ì¦í›„êµ°ì˜ ì§„ë‹¨ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"

    # ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ
    relevant_doc = """
    ëŒ€ì‚¬ì¦í›„êµ°ì€ ë‹¤ìŒ 5ê°€ì§€ ìœ„í—˜ ìš”ì¸ ì¤‘ 3ê°€ì§€ ì´ìƒì„ ì¶©ì¡±í•  ë•Œ ì§„ë‹¨ë©ë‹ˆë‹¤:
    1. ë³µë¶€ ë¹„ë§Œ (í—ˆë¦¬ë‘˜ë ˆ: ë‚¨ì„± 90cm, ì—¬ì„± 85cm ì´ìƒ)
    2. ê³ í˜ˆì•• (130/85 mmHg ì´ìƒ)
    3. ê³µë³µí˜ˆë‹¹ ì¥ì•  (100 mg/dL ì´ìƒ)
    4. ê³ ì¤‘ì„±ì§€ë°©í˜ˆì¦ (150 mg/dL ì´ìƒ)
    5. ë‚®ì€ HDL ì½œë ˆìŠ¤í…Œë¡¤ (ë‚¨ì„± 40mg/dL, ì—¬ì„± 50mg/dL ë¯¸ë§Œ)
    """

    # ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ
    irrelevant_doc = """
    ê°ê¸°ëŠ” ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼ìœ¼ë¡œ ì¸í•œ ìƒê¸°ë„ ê°ì—¼ì…ë‹ˆë‹¤.
    ì£¼ìš” ì¦ìƒìœ¼ë¡œëŠ” ì½§ë¬¼, ê¸°ì¹¨, ì¸í›„í†µ ë“±ì´ ìˆìŠµë‹ˆë‹¤.
    """

    print(f"\nì§ˆë¬¸: {query}\n")

    results = evaluator.evaluate_relevance_batch(
        query, [relevant_doc, irrelevant_doc]
    )

    print("ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ í‰ê°€:")
    print(f"  ê²°ê³¼: {results[0].relevance} (ì‹ ë¢°ë„: {results[0].confidence})")

    print("\nê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ í‰ê°€:")
    print(f"  ê²°ê³¼: {results[1].relevance} (ì‹ ë¢°ë„: {results[1].confidence})")


def test_issup():
    """ISSUP (ì§€ì§€ë„) í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ISSUP (ì§€ì§€ë„) í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    evaluator = create_evaluator()

    query = "ëŒ€ì‚¬ì¦í›„êµ°ì˜ ì§„ë‹¨ ê¸°ì¤€ì€?"

    document = """
    ëŒ€ì‚¬ì¦í›„êµ° ì§„ë‹¨ ê¸°ì¤€:
    - í—ˆë¦¬ë‘˜ë ˆ: ë‚¨ì„± 90cm, ì—¬ì„± 85cm ì´ìƒ
    - í˜ˆì••: 130/85 mmHg ì´ìƒ
    - ê³µë³µí˜ˆë‹¹: 100 mg/dL ì´ìƒ
    """

    # ì™„ì „íˆ ë’·ë°›ì¹¨ë˜ëŠ” ë‹µë³€
    fully_supported_answer = """
    ëŒ€ì‚¬ì¦í›„êµ°ì€ í—ˆë¦¬ë‘˜ë ˆ(ë‚¨ì„± 90cm, ì—¬ì„± 85cm ì´ìƒ), 
    í˜ˆì••(130/85 mmHg ì´ìƒ), ê³µë³µí˜ˆë‹¹(100 mg/dL ì´ìƒ) ë“±ì˜ 
    ê¸°ì¤€ìœ¼ë¡œ ì§„ë‹¨ë©ë‹ˆë‹¤.
    """

    # ë¶€ë¶„ì ìœ¼ë¡œ ë’·ë°›ì¹¨ë˜ëŠ” ë‹µë³€
    partially_supported_answer = """
    ëŒ€ì‚¬ì¦í›„êµ°ì€ í—ˆë¦¬ë‘˜ë ˆì™€ í˜ˆì••ìœ¼ë¡œ ì§„ë‹¨ë˜ë©°, 
    ìµœê·¼ ì—°êµ¬ì— ë”°ë¥´ë©´ ìš´ë™ ë¶€ì¡±ë„ ì£¼ìš” ì›ì¸ì…ë‹ˆë‹¤.
    """

    # ë’·ë°›ì¹¨ë˜ì§€ ì•ŠëŠ” ë‹µë³€
    no_support_answer = """
    ëŒ€ì‚¬ì¦í›„êµ°ì€ ìœ ì „ì  ìš”ì¸ì´ ê°€ì¥ í¬ë©°, 
    íŠ¹ë³„í•œ ì•½ë¬¼ ì¹˜ë£Œê°€ í•„ìš”í•©ë‹ˆë‹¤.
    """

    print(f"\nì§ˆë¬¸: {query}\n")

    print("ì™„ì „íˆ ë’·ë°›ì¹¨ë˜ëŠ” ë‹µë³€ í‰ê°€:")
    result1 = evaluator.evaluate_support_batch(
        query, [document], fully_supported_answer
    )[0]
    print(f"  ê²°ê³¼: {result1.support} (ì‹ ë¢°ë„: {result1.confidence})")

    print("\në¶€ë¶„ì ìœ¼ë¡œ ë’·ë°›ì¹¨ë˜ëŠ” ë‹µë³€ í‰ê°€:")
    result2 = evaluator.evaluate_support_batch(
        query, [document], partially_supported_answer
    )[0]
    print(f"  ê²°ê³¼: {result2.support} (ì‹ ë¢°ë„: {result2.confidence})")

    print("\në’·ë°›ì¹¨ë˜ì§€ ì•ŠëŠ” ë‹µë³€ í‰ê°€:")
    result3 = evaluator.evaluate_support_batch(query, [document], no_support_answer)[0]
    print(f"  ê²°ê³¼: {result3.support} (ì‹ ë¢°ë„: {result3.confidence})")


def test_isuse():
    """ISUSE (ìœ ìš©ì„±) í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ISUSE (ìœ ìš©ì„±) í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    evaluator = create_evaluator()

    query = "ëŒ€ì‚¬ì¦í›„êµ°ì„ ì˜ˆë°©í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"

    # ë§¤ìš° ìœ ìš©í•œ ë‹µë³€ (5ì  ì˜ˆìƒ)
    excellent_answer = """
    ëŒ€ì‚¬ì¦í›„êµ° ì˜ˆë°©ì„ ìœ„í•´ì„œëŠ”:
    1. ê·œì¹™ì ì¸ ìš´ë™ (ì£¼ 150ë¶„ ì´ìƒ)
    2. ê±´ê°•í•œ ì‹ìŠµê´€ (ì±„ì†Œ, ê³¼ì¼ ì¤‘ì‹¬)
    3. ì²´ì¤‘ ê´€ë¦¬ (ì ì • BMI ìœ ì§€)
    4. ê¸ˆì—° ë° ì ˆì£¼
    5. ì •ê¸°ì ì¸ ê±´ê°•ê²€ì§„
    ì„ ì‹¤ì²œí•˜ì‹œëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
    """

    # ë³´í†µ ë‹µë³€ (3ì  ì˜ˆìƒ)
    average_answer = """
    ëŒ€ì‚¬ì¦í›„êµ°ì„ ì˜ˆë°©í•˜ë ¤ë©´ ê±´ê°•í•œ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
    ìš´ë™ê³¼ ì‹ì´ì¡°ì ˆì„ í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
    """

    # ìœ ìš©í•˜ì§€ ì•Šì€ ë‹µë³€ (1-2ì  ì˜ˆìƒ)
    poor_answer = """
    ëŒ€ì‚¬ì¦í›„êµ°ì€ ë³µì¡í•œ ì§ˆí™˜ì…ë‹ˆë‹¤.
    """

    print(f"\nì§ˆë¬¸: {query}\n")

    documents = [
        "ëŒ€ì‚¬ì¦í›„êµ° ì˜ˆë°©ì—ëŠ” ê·œì¹™ì ì¸ ìš´ë™, ê±´ê°•í•œ ì‹ìŠµê´€, ì²´ì¤‘ ê´€ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "ëŒ€ì‚¬ì¦í›„êµ°ì€ ë³µë¶€ ë¹„ë§Œê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤.",
    ]

    print("ë§¤ìš° ìœ ìš©í•œ ë‹µë³€ í‰ê°€:")
    result1 = evaluator.evaluate_answer_quality(query, excellent_answer, documents)
    print(f"  ì ìˆ˜: {result1.usefulness_score}/5 (ì‹ ë¢°ë„: {result1.usefulness_confidence})")

    print("\në³´í†µ ë‹µë³€ í‰ê°€:")
    result2 = evaluator.evaluate_answer_quality(query, average_answer, documents)
    print(f"  ì ìˆ˜: {result2.usefulness_score}/5 (ì‹ ë¢°ë„: {result2.usefulness_confidence})")

    print("\nìœ ìš©í•˜ì§€ ì•Šì€ ë‹µë³€ í‰ê°€:")
    result3 = evaluator.evaluate_answer_quality(query, poor_answer, documents)
    print(f"  ì ìˆ˜: {result3.usefulness_score}/5 (ì‹ ë¢°ë„: {result3.usefulness_confidence})")


def test_overall_evaluation():
    """ì „ì²´ í‰ê°€ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ì „ì²´ í‰ê°€ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    evaluator = create_evaluator()

    query = "ëŒ€ì‚¬ì¦í›„êµ°ì˜ ì£¼ìš” ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?"

    documents = [
        "ëŒ€ì‚¬ì¦í›„êµ°ì˜ ì£¼ìš” ì›ì¸ì€ ë¹„ë§Œ, ìš´ë™ ë¶€ì¡±, ë¶ˆê·œì¹™í•œ ì‹ìŠµê´€ì…ë‹ˆë‹¤.",
        "ëŒ€ì‚¬ì¦í›„êµ°ì€ ë³µë¶€ ë¹„ë§Œê³¼ ì¸ìŠë¦° ì €í•­ì„±ì´ í•µì‹¬ ì›ì¸ì…ë‹ˆë‹¤.",
        "ê°ê¸° ì˜ˆë°©ì„ ìœ„í•´ì„œëŠ” ì†ì”»ê¸°ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.",  # ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ
    ]

    print(f"\nì§ˆë¬¸: {query}")
    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")

    # 1ë‹¨ê³„: ë¬¸ì„œ í‰ê°€
    overall_eval = evaluator.assess_retrieval_quality(query, documents)

    print(f"\nì™¸ë¶€ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€: {overall_eval.should_retrieve_external}")
    print(f"ì‚¬ìœ : {overall_eval.reason}")

    print("\në¬¸ì„œë³„ í‰ê°€:")
    for i, doc_eval in enumerate(overall_eval.document_evaluations, 1):
        print(f"  ë¬¸ì„œ {i}: {doc_eval.relevance.relevance}")

    # 2ë‹¨ê³„: ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ê°€ì •: ë‹µë³€ì´ ìƒì„±ë˜ì—ˆë‹¤ê³  ê°€ì •)
    generated_answer = """
    ëŒ€ì‚¬ì¦í›„êµ°ì˜ ì£¼ìš” ì›ì¸ì€ ë³µë¶€ ë¹„ë§Œ, ìš´ë™ ë¶€ì¡±, ë¶ˆê·œì¹™í•œ ì‹ìŠµê´€ì…ë‹ˆë‹¤.
    íŠ¹íˆ ì¸ìŠë¦° ì €í•­ì„±ì´ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """

    print(f"\nìƒì„±ëœ ë‹µë³€:")
    print(f"  {generated_answer}")

    answer_quality = evaluator.assess_answer_quality(
        query, generated_answer, documents[:2]  # ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë§Œ ì‚¬ìš©
    )

    fully_supported = sum(
        1 for item in answer_quality.support_results if item.support == "fully_supported"
    )
    print(f"\në‹µë³€ í’ˆì§ˆ í‰ê°€:")
    print(f"  ìœ ìš©ì„±: {answer_quality.usefulness_score}/5")
    print(f"  ì™„ì „íˆ ë’·ë°›ì¹¨ë˜ëŠ” ë¬¸ì„œ ìˆ˜: {fully_supported}")
    print(f"  ì¬ìƒì„± í•„ìš”: {answer_quality.should_regenerate}")


if __name__ == "__main__":
    from dotenv import load_dotenv

    load_dotenv()
    print("\nğŸ” Self-RAG í‰ê°€ì í…ŒìŠ¤íŠ¸ ì‹œì‘\n")

    try:
        test_retrieve()
        test_isrel()
        test_issup()
        test_isuse()
        test_overall_evaluation()

        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
